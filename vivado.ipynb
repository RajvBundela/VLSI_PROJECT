{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Mounting The Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90cFwK72Z0JE",
        "outputId": "43577d22-5b72-4f5d-baa5-75a2ed40979e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0bS2qQpu2hl"
      },
      "outputs": [],
      "source": [
        "# Importing the libraries into the file.\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import np_utils\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "from mlxtend.evaluate import confusion_matrix\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "from matplotlib.pyplot import subplots\n",
        "from keras.callbacks import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Reading the csv file which has stored the pixel value of all the images of dataset to be used\n",
        "dataset = pd.read_csv(\"/content/drive/MyDrive/A_Z Handwritten Data.csv\").astype('float32')\n",
        "dataset.rename(columns={'0':'label'}, inplace=True)\n",
        "\n",
        "#Pixel values of Images \n",
        "x= dataset.drop('label',axis = 1)\n",
        "\n",
        "#Seperating the column which has labels of all the data items\n",
        "y = dataset['label']"
      ],
      "metadata": {
        "id": "Ec_X8Di_aO2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We are spliting the dataset into train,test and validation dataset.\n",
        "\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y)\n",
        "\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3)\n",
        "x_train,inputs_validation,y_train,targets_validation=train_test_split(x_train,y_train,test_size=0.2)\n",
        "\n",
        "#Scaling the values of the pixels in the different dataset. \n",
        "standard_scaler = MinMaxScaler()\n",
        "standard_scaler.fit(x_train)\n",
        "\n",
        "x_train = standard_scaler.transform(x_train)\n",
        "x_test = standard_scaler.transform(x_test)\n",
        "\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)"
      ],
      "metadata": {
        "id": "U4TPerVnatO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining the Architecture Of the ANN Model to be used. \n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(layers.Dense(100, activation=\"relu\" , input_dim = x_train.shape[1]))\n",
        "model.add(layers.Dense(len(y.unique()), activation=\"softmax\"))\n",
        "adam = keras.optimizers.Adam(learning_rate=0.001,decay=1e-6)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=adam , metrics = ['Accuracy'])"
      ],
      "metadata": {
        "id": "xE6pBwsMb6vX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training the model with the training dataset\n",
        "\n",
        "model.fit(x_train,y_train,epochs=5)"
      ],
      "metadata": {
        "id": "kITj3ZFjFe39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluating the Model on Test Dataset\n",
        "model.evaluate(x_test,y_test)"
      ],
      "metadata": {
        "id": "fQ0SXPiNXPUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Storing the predicted values of data items in test dataset.\n",
        "y_pred=model.predict(x_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruF0ESGEgZvv",
        "outputId": "3555f045-a43a-4b2e-f372-3a1398643c2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2910/2910 [==============================] - 5s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Function To get appropriate Alphabet based on the predictions of the Model\n",
        "#The model returns an array of 26 size describing the probabilities of each character.\n",
        "#We take the character with max probability as the predicted result for that sample image.\n",
        "\n",
        "def get_alphabet(y):\n",
        "  s=\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
        "  val=[0]*len(y)\n",
        "  for j in range (len(y)):\n",
        "   val[j]=s[np.argmax(y[j])]\n",
        "  return val"
      ],
      "metadata": {
        "id": "-pJzxhyT046I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Saving the model in the drive to later extract weight from it.\n",
        "\n",
        "model.save_weights('gfgModelWeights.h5')\n",
        "print('Model Saved!')"
      ],
      "metadata": {
        "id": "eTsbCoxOSv4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calling the get_alphabet() to get the respective value of alphabets.\n",
        "y_test=get_alphabet(y_test)\n",
        "y_pred=get_alphabet(y_pred)"
      ],
      "metadata": {
        "id": "GHmC37fK0pew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Displaying the confusion matrix based upon the values of Predicted value and actual Values Of Test Dataset\n",
        "\n",
        "disp = metrics.ConfusionMatrixDisplay.from_predictions(y_test, y_pred)\n",
        "fig = disp.ax_.get_figure() \n",
        "fig.set_figwidth(10)\n",
        "fig.set_figheight(10)  "
      ],
      "metadata": {
        "id": "SIzyYw6lyhAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loc = \"./\" # save location\n",
        "shape_dict = {} # (layer name:shape) save dictionary"
      ],
      "metadata": {
        "id": "MPdRI2BUfh9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To print the value of weights in each layer of model.\n",
        "for layer in model.layers:\n",
        "  print(layer.get_weights())\n",
        "  print(\"layer finished\")"
      ],
      "metadata": {
        "id": "uMBjwtMOfksd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in model.layers:\n",
        "    if layer.get_weights() != []:\n",
        "        shape_dict[layer.name] = np.shape(layer.get_weights()[2]) # No bias, only weights saved (initialized bias=0)\n",
        "        np.savetxt(loc + layer.name +\".csv\", layer.get_weights()[2].flatten(), delimiter=\",\")"
      ],
      "metadata": {
        "id": "SQM3vxUGHm1r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}